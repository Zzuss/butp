const { createClient } = require('@supabase/supabase-js')
const XLSX = require('xlsx')
const fs = require('fs')
const path = require('path')
const axios = require('axios')
const winston = require('winston')
const cron = require('node-cron')
require('dotenv').config()

// é…ç½®æ—¥å¿—
const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json()
  ),
  transports: [
    new winston.transports.File({ filename: 'logs/error.log', level: 'error' }),
    new winston.transports.File({ filename: 'logs/combined.log' }),
    new winston.transports.Console({
      format: winston.format.simple()
    })
  ]
})

// Supabaseé…ç½®
const supabaseUrl = process.env.SUPABASE_URL
const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY
const supabase = createClient(supabaseUrl, supabaseKey)

// é…ç½®
const BATCH_SIZE = parseInt(process.env.BATCH_SIZE) || 1000
const TEMP_DIR = path.join(__dirname, process.env.TEMP_DIR || 'temp')
const MAX_CONCURRENT_TASKS = parseInt(process.env.MAX_CONCURRENT_TASKS) || 2

// ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨
if (!fs.existsSync(TEMP_DIR)) {
  fs.mkdirSync(TEMP_DIR, { recursive: true })
}

if (!fs.existsSync('logs')) {
  fs.mkdirSync('logs', { recursive: true })
}

class ImportWorker {
  constructor() {
    this.isProcessing = false
    this.currentTasks = new Set()
  }

  // å¯åŠ¨å·¥ä½œè¿›ç¨‹
  async start() {
    logger.info('ğŸš€ ECSå¯¼å…¥å·¥ä½œè¿›ç¨‹å¯åŠ¨')
    
    // ç«‹å³æ£€æŸ¥ä¸€æ¬¡
    await this.processQueue()
    
    // æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡é˜Ÿåˆ—
    cron.schedule('*/30 * * * * *', async () => {
      if (!this.isProcessing && this.currentTasks.size < MAX_CONCURRENT_TASKS) {
        await this.processQueue()
      }
    })

    // æ¯å°æ—¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    cron.schedule('0 * * * *', async () => {
      await this.cleanupTempFiles()
    })

    logger.info('âœ… å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨')
  }

  // å¤„ç†é˜Ÿåˆ—
  async processQueue() {
    if (this.isProcessing) {
      return
    }

    try {
      this.isProcessing = true
      
      // æŸ¥æ‰¾å¾…å¤„ç†çš„ä»»åŠ¡
      const { data: tasks, error } = await supabase
        .from('import_tasks')
        .select('*')
        .eq('status', 'pending')
        .order('created_at', { ascending: true })
        .limit(MAX_CONCURRENT_TASKS - this.currentTasks.size)

      if (error) {
        throw error
      }

      if (!tasks || tasks.length === 0) {
        return
      }

      logger.info(`ğŸ“‹ æ‰¾åˆ° ${tasks.length} ä¸ªå¾…å¤„ç†ä»»åŠ¡`)

      // å¹¶å‘å¤„ç†ä»»åŠ¡
      const promises = tasks.map(task => this.processTask(task))
      await Promise.all(promises)

    } catch (error) {
      logger.error('å¤„ç†é˜Ÿåˆ—å¤±è´¥:', error)
    } finally {
      this.isProcessing = false
    }
  }

  // å¤„ç†å•ä¸ªä»»åŠ¡
  async processTask(task) {
    const taskId = task.id
    this.currentTasks.add(taskId)

    try {
      logger.info(`ğŸ”„ å¼€å§‹å¤„ç†ä»»åŠ¡: ${taskId}`)

      // æ ‡è®°ä»»åŠ¡ä¸ºå¤„ç†ä¸­
      await supabase
        .from('import_tasks')
        .update({ status: 'processing' })
        .eq('id', taskId)

      // æ¸…ç©ºå½±å­è¡¨
      await this.clearShadowTable()

      // è·å–ä»»åŠ¡çš„æ–‡ä»¶åˆ—è¡¨
      const { data: files, error: filesError } = await supabase
        .from('import_file_details')
        .select('*')
        .eq('task_id', taskId)
        .eq('status', 'pending')

      if (filesError) {
        throw filesError
      }

      if (!files || files.length === 0) {
        throw new Error('æ²¡æœ‰æ‰¾åˆ°å¾…å¤„ç†çš„æ–‡ä»¶')
      }

      let totalRecords = 0
      let importedRecords = 0
      let hasErrors = false

      // å¤„ç†æ¯ä¸ªæ–‡ä»¶
      for (const file of files) {
        try {
          const result = await this.processFile(file)
          totalRecords += result.totalRecords
          importedRecords += result.importedRecords
        } catch (fileError) {
          logger.error(`å¤„ç†æ–‡ä»¶å¤±è´¥: ${file.file_name}`, fileError)
          hasErrors = true
          
          // æ ‡è®°æ–‡ä»¶å¤„ç†å¤±è´¥
          await supabase
            .from('import_file_details')
            .update({
              status: 'failed',
              error_message: fileError.message
            })
            .eq('id', file.id)
        }
      }

      // å®Œæˆä»»åŠ¡
      if (hasErrors || importedRecords === 0) {
        await this.failTask(taskId, totalRecords, importedRecords, 'éƒ¨åˆ†æ–‡ä»¶å¤„ç†å¤±è´¥')
      } else {
        await this.completeTask(taskId, totalRecords, importedRecords)
      }

    } catch (error) {
      logger.error(`ä»»åŠ¡å¤„ç†å¤±è´¥: ${taskId}`, error)
      await this.failTask(taskId, 0, 0, error.message)
    } finally {
      this.currentTasks.delete(taskId)
    }
  }

  // å¤„ç†å•ä¸ªæ–‡ä»¶
  async processFile(fileDetail) {
    logger.info(`ğŸ“„ å¤„ç†æ–‡ä»¶: ${fileDetail.file_name}`)

    // æ ‡è®°æ–‡ä»¶ä¸ºå¤„ç†ä¸­
    await supabase
      .from('import_file_details')
      .update({ 
        status: 'processing',
        processed_at: new Date().toISOString()
      })
      .eq('id', fileDetail.id)

    // è·å–æœ¬åœ°æ–‡ä»¶
    const filePath = await this.getLocalFile(fileDetail)
    
    // è¯»å–Excelæ–‡ä»¶
    const workbook = XLSX.readFile(filePath)
    const sheetName = workbook.SheetNames[0]
    const worksheet = workbook.Sheets[sheetName]
    const jsonData = XLSX.utils.sheet_to_json(worksheet)

    if (jsonData.length === 0) {
      throw new Error('æ–‡ä»¶ä¸­æ²¡æœ‰æ•°æ®')
    }

    // å¤„ç†æ•°æ®
    const processedData = jsonData.map(row => this.mapExcelRow(row))
    
    // åˆ†æ‰¹å¯¼å…¥
    let importedCount = 0
    for (let i = 0; i < processedData.length; i += BATCH_SIZE) {
      const batch = processedData.slice(i, i + BATCH_SIZE)
      
      const { error } = await supabase
        .from('academic_results_old')
        .insert(batch)

      if (error) {
        throw new Error(`æ‰¹æ¬¡å¯¼å…¥å¤±è´¥: ${error.message}`)
      }

      importedCount += batch.length
      logger.info(`âœ… æ‰¹æ¬¡å¯¼å…¥æˆåŠŸ: ${importedCount}/${processedData.length}`)
    }

    // æ›´æ–°æ–‡ä»¶çŠ¶æ€
    await supabase
      .from('import_file_details')
      .update({
        status: 'completed',
        records_count: jsonData.length,
        imported_count: importedCount
      })
      .eq('id', fileDetail.id)

    // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if (fs.existsSync(filePath)) {
      fs.unlinkSync(filePath)
    }

    return {
      totalRecords: jsonData.length,
      importedRecords: importedCount
    }
  }

  // è·å–æœ¬åœ°æ–‡ä»¶ï¼ˆæ–‡ä»¶å·²é€šè¿‡ä¸Šä¼ æœåŠ¡å­˜å‚¨åœ¨ECSï¼‰
  async getLocalFile(fileDetail) {
    // å°è¯•å¤šç§æ–‡ä»¶åæ ¼å¼
    const possibleFiles = [
      `${fileDetail.file_id}.xlsx`,
      `${fileDetail.file_id}.xls`,
      fileDetail.file_name // å¦‚æœæœ‰åŸå§‹æ–‡ä»¶å
    ]
    
    for (const fileName of possibleFiles) {
      const filePath = path.join(TEMP_DIR, fileName)
      
      if (fs.existsSync(filePath)) {
        logger.info(`âœ… æ‰¾åˆ°æœ¬åœ°æ–‡ä»¶: ${fileName}`)
        return filePath
      }
    }
    
    // å¦‚æœæ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
    logger.error(`âŒ æ‰¾ä¸åˆ°æœ¬åœ°æ–‡ä»¶: ${fileDetail.file_id}`)
    logger.info(`   æŸ¥æ‰¾çš„æ–‡ä»¶å: ${possibleFiles.join(', ')}`)
    logger.info(`   æŸ¥æ‰¾ç›®å½•: ${TEMP_DIR}`)
    
    // åˆ—å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ç”¨äºè°ƒè¯•
    try {
      const dirFiles = fs.readdirSync(TEMP_DIR)
      logger.info(`   ç›®å½•ä¸­çš„æ–‡ä»¶: ${dirFiles.join(', ')}`)
    } catch (error) {
      logger.error(`   æ— æ³•è¯»å–ç›®å½•: ${error.message}`)
    }
    
    throw new Error(`æ‰¾ä¸åˆ°æ–‡ä»¶: ${fileDetail.file_id}`)
  }

  // æ•°æ®æ˜ å°„
  mapExcelRow(row) {
    return {
      SNH: row.SNH || null,
      Semester_Offered: row.Semester_Offered || row.Semester || null,
      Current_Major: row.Current_Major || row.Major || null,
      Course_ID: row.Course_ID || row.Course_Code || null,
      Course_Name: row.Course_Name || null,
      Grade: row.Grade || null,
      Grade_Remark: row.Grade_Remark || null,
      Course_Type: row.Course_Type || null,
      Course_Attribute: row.Course_Attribute || null,
      Hours: row.Hours || null,
      Credit: row.Credit ? parseFloat(row.Credit) : null,
      Offering_Unit: row.Offering_Unit || null,
      Tags: row.Tags || null,
      Description: row.Description || null,
      Exam_Type: row.Exam_Type || null,
      Assessment_Method: row['Assessment_Method '] || row.Assessment_Method || null,
      year: row.year ? parseInt(row.year) : null,
    }
  }

  // æ¸…ç©ºå½±å­è¡¨
  async clearShadowTable() {
    logger.info('ğŸ§¹ æ¸…ç©ºå½±å­è¡¨')
    try {
      const { error } = await supabase.rpc('truncate_results_old')
      if (error) {
        throw error
      }
    } catch (error) {
      // å¦‚æœRPCå¤±è´¥ï¼Œä½¿ç”¨DELETE
      const { error: deleteError } = await supabase
        .from('academic_results_old')
        .delete()
        .neq('SNH', 'dummy_value_that_should_not_exist')
      
      if (deleteError) {
        throw deleteError
      }
    }
  }

  // å®Œæˆä»»åŠ¡
  async completeTask(taskId, totalRecords, importedRecords) {
    logger.info(`ğŸ‰ ä»»åŠ¡å®Œæˆ: ${taskId}`)
    
    // æ‰§è¡ŒåŸå­äº¤æ¢
    const { error: swapError } = await supabase.rpc('swap_results_with_old')
    if (swapError) {
      throw new Error(`åŸå­äº¤æ¢å¤±è´¥: ${swapError.message}`)
    }

    // æ›´æ–°ä»»åŠ¡çŠ¶æ€
    await supabase
      .from('import_tasks')
      .update({
        status: 'completed',
        total_records: totalRecords,
        imported_records: importedRecords,
        completed_at: new Date().toISOString()
      })
      .eq('id', taskId)
  }

  // ä»»åŠ¡å¤±è´¥
  async failTask(taskId, totalRecords, importedRecords, errorMessage) {
    logger.error(`âŒ ä»»åŠ¡å¤±è´¥: ${taskId} - ${errorMessage}`)
    
    // æ¸…ç©ºå½±å­è¡¨ä½œä¸ºå›æ»š
    await this.clearShadowTable()

    // æ›´æ–°ä»»åŠ¡çŠ¶æ€
    await supabase
      .from('import_tasks')
      .update({
        status: 'failed',
        total_records: totalRecords,
        imported_records: importedRecords,
        error_message: errorMessage,
        completed_at: new Date().toISOString()
      })
      .eq('id', taskId)
  }

  // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
  async cleanupTempFiles() {
    try {
      const files = fs.readdirSync(TEMP_DIR)
      const now = Date.now()
      const maxAge = 24 * 60 * 60 * 1000 // 24å°æ—¶

      for (const file of files) {
        const filePath = path.join(TEMP_DIR, file)
        const stats = fs.statSync(filePath)
        
        if (now - stats.mtime.getTime() > maxAge) {
          fs.unlinkSync(filePath)
          logger.info(`ğŸ—‘ï¸ æ¸…ç†è¿‡æœŸæ–‡ä»¶: ${file}`)
        }
      }
    } catch (error) {
      logger.error('æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥:', error)
    }
  }
}

// å¯åŠ¨å·¥ä½œè¿›ç¨‹
const worker = new ImportWorker()
worker.start().catch(error => {
  logger.error('å¯åŠ¨å·¥ä½œè¿›ç¨‹å¤±è´¥:', error)
  process.exit(1)
})

// ä¼˜é›…å…³é—­
process.on('SIGINT', () => {
  logger.info('ğŸ›‘ æ”¶åˆ°å…³é—­ä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…å…³é—­...')
  process.exit(0)
})

process.on('SIGTERM', () => {
  logger.info('ğŸ›‘ æ”¶åˆ°ç»ˆæ­¢ä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…å…³é—­...')
  process.exit(0)
})
